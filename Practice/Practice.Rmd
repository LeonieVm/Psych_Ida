---
title: "What is Psychometrics? Psychometric Evaluation of (Intensive) Longitudinal Data"
author: Leonie V.D.E. Vogelsmeier & Joran Jongerling
output:
  bookdown::html_document2:
     css: "style.css"
     number_sections: yes
     toc: true
     toc_float:
          collapsed: true
     df_print: paged # for easy display of data
---

```{r klippy, echo = FALSE}
# Create a button for copy pasting code chunk to clipboard
klippy::klippy(position = c("top", "right"))

printAnswers = FALSE

```

# General instructions {-}

During this lab session, we will take you through the steps of assessing measurement invariance and adequacy of model fit for cross-sectional, panel, and intensive longitudinal data. Note that invariance assessment for intensive longitudinal data is still difficult and requires special software(packages). We have some code for you to perform Latent Markov Factor Analysis using the `lmfa` package at the end of this script. We are currently working on implementing the cross-classified factor analysis in R (using Stan in the background), but this is still work in progress. You can, however, already perform the analysis in MPlus (code can be found here: https://osf.io/f83km/ ) 

To answer the questions in this practice sheet:

1. Download the material under [Practice](https://github.com/LeonieVm/Psych_Ida).
2. Unzip the archive.

You will need the two **datasets** `Data_Cross_Panel` and `Data_ILD`, which are automatically in your global environment once you open the workspace `Workspace_IOPS.RData` via the load-workspace button or with the command `load("[Path on your computer]/Workspace_IOPS.RData")`

<!-- R Markdown set up code -->
```{r global_options, include = FALSE}
# Do you want solutions or not? TRUE = yes, FALSE = NO
if(!exists("solutions")){
     solutions <- TRUE
}

# Load knitr package to change options
library(knitr)

# Set default options for the chunks
opts_chunk$set(
    include = solutions, # solutions or not?
    echo = TRUE,
    warning = FALSE,
    message = FALSE,
    comment = ""
)

# Set the width of the console output to a large number (for sidewise scroll)
options(width = 1e3)

# Define a function to change the color of the reported text
colorize <- function(x, color) {
     if (knitr::is_latex_output()) {
          sprintf("\\textcolor{%s}{%s}", color, x)
     } else if (knitr::is_html_output()) {
          sprintf(
               "<span style='color: %s;'>%s</span>", color,
               x
          )
     } else {
          x
     }
}

# Define a function for reporting text answers
print.response <- function(paragraphs.vector = c("Paragraph 1", "Paragraph 2"), color = "#007FB3") {
     # Colorize every paragraph
     text <- sapply(paragraphs.vector, colorize, color = color)

     # Cat
     cat(text, sep = "\n\n")
}

```

# Preliminaries

If you have not already done so, use the `install.packages` function to install the packages `semTools` and `devtools` and via `devtools`, install the package `lmfa`.
Important note: Please check if you have the package `igraph` installed. If yes, please remove it (e.g., under the panel "Packages"). We need an older version because of a recent bug.
```{r, eval = FALSE}
# Install the packages if you don't have them yet
install.packages("semTools")
install.packages("devtools")

# Trick to install older igraph version
require(devtools)
install_version("igraph", version = "1.3.0", repos = "http://cran.us.r-project.org")

# Continue installing lmfa
devtools::install_github("leonievm/lmfa")

# Trick if something fails with semTools:
detach("package:semTools", unload = TRUE)
detach("package:lavaan", unload = TRUE)

```

Then load the packages and the workspace.
```{r}
# Load packages
library(semTools)
library(devtools)
library(lmfa)

# Read the workspace we will need for this session
load("Workspace_IOPS.RData")

# Read data that is part of the ESM package
data("ESM")

```

# Cross-Sectional Data

Use the `Data_Cross_Panel` data to answer the questions in this section. We start by defining a model. Subsequently, we assess invariance. Finally, we retrieve the reliability.

Note: for an explanation of the model syntax (including the different symbols), see https://lavaan.ugent.be/tutorial/cfa.html

Quick summary:

* Regression of Y on X:                `Y ~ X`

* Variance of Y specified as:          `Y ~~ Y`

* Covariance between X and Y:          `X ~~ Y`

* Intercept given by:                  `X ~ 1`

* Loading of item on factor            `F1 =~ X`

## - Write syntax for the factor model for the cross-sectional data.

```{r}
CM_factor_T1 <- '

# Define the latent factors.
CM_1 =~ NA*T1_CM_01 +lambda1*T1_CM_01 + T1_CM_02 + T1_CM_03 + T1_CM_04 + 
        T1_CM_05 + T1_CM_06 + T1_CM_07 + T1_CM_08

# Intercepts
T1_CM_01 ~ i1*1
T1_CM_02 ~ 1
T1_CM_03 ~ 1
T1_CM_04 ~ 1
T1_CM_05 ~ 1
T1_CM_06 ~ 1
T1_CM_07 ~ 1
T1_CM_08 ~ 1

# Unique Variances
T1_CM_01 ~~ T1_CM_01
T1_CM_02 ~~ T1_CM_02
T1_CM_03 ~~ T1_CM_03
T1_CM_04 ~~ T1_CM_04
T1_CM_05 ~~ T1_CM_05
T1_CM_06 ~~ T1_CM_06
T1_CM_07 ~~ T1_CM_07
T1_CM_08 ~~ T1_CM_08

# Latent Variable Means
CM_1 ~ 0*1

# Latent Variable Variances and Covariance
CM_1 ~~ 1*CM_1
'
```

## - Use the `cfa` function to fit the model to the data and set the argument `mimic` equal to "mplus".

```{r}
# Model fitting
fit_CM_T1 <- cfa(CM_factor_T1, data = Data_Cross_Panel, mimic = "mplus")

```

## - Use the `summary` function. Does the model fit well to the data?
Guidelines:



Look at **CFI** (should be > .90), **RMSEA** (should be < .08), 
**SRMR** (should be < .08), and **chi-square** (should be non-significant).

* The CFI (Comparative Fit Index), RMSEA (Root Mean Squared Error of 
Approximation), and SRMR (Standardized Root Mean Squared Error) are 
Approximate Fit Indices. They do not correct for sampling variability,
have no clear cut-offs for good fit, and assume multivariate normal data.
They are more qualitative measures of fit.


    + CFI: Looks at the improvement in fit relative to the independence model 
       which states that every variable has a mean and variance, but all are 
       unrelated (correlations are all 0).
       
    + RMSEA: Tests for close fit between data (observed (co)variances) 
       and model implied relations.
       
    + SRMR:  Looks at the (mean) covariance residuals. 


* Chi-square: Actual test with corresponding null-hypothesis.
It tests for the null-hypothesis of perfect fit (in the population), that is,
the model perfectly recovers the observed (co)variances, which
might be a little unrealistic. It is (unnecessarily) strict (especially for 
large sample size), assumes multivariate normality, and assumes a random 
sample. Moreover, it doesn't work well in case where there are high correlations 
and/or high percentages of unique variance. 
It also tends to miss misfit in single parameters (e.g., a single 
large covariance residual) and patterns of smaller misfit (many small 
residuals), AND a non-significant result does NOT mean a good model.

```{r}
# Getting the results
summary(fit_CM_T1, fit.measures = TRUE)

```

```{r answer1, results = 'asis', echo = FALSE, include = printAnswers}

print.response("The CFI is larger than .90 and the SRMR is smaller than .08. 
The point-estimate of the RMSEA is also smaller than .08, but the 95% confidence 
interval for this fit measure goes up all the way to .092. Finally, the 
chi-square test is significant. Normally, we would like at least three of the 
four indices to show good modelfit, but here only two of the indices do so,
while a third indicates that modelfit is maybe ok. We therefore conclude 
that modelfit is sufficient but not good.")

```

## - Generate code for checking configural invariance between groups. Can you figure out what the `c(NA, NA)*T1_CM_0X` implies?

```{r}
# Generate code for checking configural invariance between groups
mod.configural  <- semTools::measEq.syntax(configural.model = CM_factor_T1, 
                                         data = Data_Cross_Panel,
                                         group = "Gender", mimic = "mplus")

# Inspect the generated code
cat(as.character(mod.configural))
```
```{r answer2, results = 'asis', echo = FALSE, include = printAnswers}

print.response("As explained in the lavaan manual, 'there is no need [...] to set the factor loading
of its first indicator[s per group] equal to one [because we already fix the factor variances to 1]. To force [the] factor loading[s] to be free, we pre-multiply it with NA, as a hint to lavaan that the value of this parameter is ‘missing’ and therefore still unknown.'")

```

## - Generate code for checking loading invariance between groups.

```{r}
# Generate code for checking invariance of loadings
mod.loadings  <- semTools::measEq.syntax(configural.model = CM_factor_T1, 
                                           data = Data_Cross_Panel,
                                           group.equal = c("loadings"),
                                           group = "Gender", mimic = "mplus")

# Inspect the generated code
cat(as.character(mod.loadings))
```

## - Generate code for checking intercept invariance between groups.

```{r}
# Generate code for checking invariance of intercepts and loadings
mod.int.load <- semTools::measEq.syntax(configural.model = CM_factor_T1, 
                                           data = Data_Cross_Panel,
                                           group.equal = c("intercepts", 
                                                           "loadings"),
                                           group = "Gender", mimic = "mplus")

# Inspect the generated code
cat(as.character(mod.int.load))
```

## - Fit the three models to the data.

```{r}
# Fit the configural invariance model
fit.configural <- cfa(as.character(mod.configural), data = Data_Cross_Panel, 
                      group = "Gender", mimic = "mplus")

# Fit the loading invariance model
fit.loadings <- cfa(as.character(mod.loadings), data = Data_Cross_Panel, 
                    group = "Gender", mimic = "mplus")

# Fit the intercept and loading invariance model
fit.int.load <- cfa(as.character(mod.int.load), data = Data_Cross_Panel, 
                    group = "Gender", mimic = "mplus")

```

## - Does configural invariance hold?

```{r}
# Check model fit using CFI, RMSEA, SRMR, and Chi-square again
summary(fit.configural, fit.measures = TRUE)
```

```{r answer3, results = 'asis', echo = FALSE, include = printAnswers}

print.response("Yes, configural invariance holds because the  CFI is larger than .90 and the SRMR is smaller than .08. 
The point-estimate of the RMSEA is also smaller than .08, but again the 95% confidence 
interval for this fit measure does include the value of .08 (and higher, up to .096). Finally, the 
chi-square test is significant. We therefore conclude 
that modelfit of the configural invariance model is sufficient.")

```

## - Now compare the different models successively, using the guidelines below.

* More than 20 observations per group: Change should not be larger than -.01 for **CFI**, 0.015 for **RMSEA**, and 0.03 for **SRMR** (*loading invariance*) or .015 for **SRMR** (*intercept invariance*). Difference in chi-square should be non-significant

* 10-20 observations per group: All as above but: Change should not be larger than -.02 for **CFI** and 0.03 **RMSEA** for *loading invariance*. 

* Less than 10 observations per group: Invariance testing is not a good idea.

## - Does loading invariance hold?

```{r}
Check.Load <- semTools::compareFit(fit.configural, fit.loadings) 
summary(Check.Load)

```

```{r answer4, results = 'asis', echo = FALSE, include = printAnswers}

print.response("Yes, loading invariance holds because the chi-square difference
               test between the configural model and the model with fixed 
               loadings across groups is non-significant. In addition, the change 
               in CFI, RMSEA, and SRMR are .001, -.006, and .018 respectively.
               This is all within the guidelines specified above. Importantly,
               we also see that the model with fixed loadings has adequate
               fit to the data since the CFI is larger than .90, and the RMSEA
               and SRMR are both smaller than .08. If the fit of the model
               with fixed loadings had been insufficient, you should not 
               conclude that there is loading invariance even if the differences
               with the configural model are all within the boundaries specified
               above.")

```

## - Does intercept invariance hold?

```{r}
Check.Int <- semTools::compareFit(fit.loadings, fit.int.load)
summary(Check.Int)

```

```{r answer5, results = 'asis', echo = FALSE, include = printAnswers}

print.response("No, intercept invariance does not hold because the chi-square 
               difference test is significant and the change in CFI is larger 
               than .01. The changes in RMSEA and SRMR are okay, but two out of 
               the four measures indicate that fixing intercepts acorss groups
               leads to significantly worse fit.")

```

## - Check the differences in intercepts across groups. Which difference is the largest?

```{r}
# Difference in groups in intercepts
parameterEstimates(fit.loadings)[9:16,7] - 
  parameterEstimates(fit.loadings)[35:42,7]

which(
  (parameterEstimates(fit.loadings)[9:16,7] - 
     parameterEstimates(fit.loadings)[35:42,7]) 
  == min(parameterEstimates(fit.loadings)[9:16,7] - 
           parameterEstimates(fit.loadings)[35:42,7]))

```

```{r answer6, results = 'asis', echo = FALSE, include = printAnswers}

print.response("The largest difference is in the intercept of item 5. Note that the indices in the code above are specific for this factor model. If you have a different model, you need to use different indices.")

```

## - Adjust the model by allowing for partial non-invariance of item 5 and fit it to the data.

```{r}
mod.intercepts.part <- semTools::measEq.syntax(configural.model= CM_factor_T1, 
                                          data = Data_Cross_Panel,
                                          group.equal = c("loadings",
                                                          "intercepts"),
                                          group.partial = c("T1_CM_05 ~ 1"),
                                          group ="Gender", mimic = "mplus")

# Inspect the generated code
cat(as.character(mod.intercepts.part))

# Fit the intercept invariance model
fit.intercepts.part <- cfa(as.character(mod.intercepts.part), data = Data_Cross_Panel, 
                      group ="Gender", mimic = "mplus")

```

## - Does partial intercept invariance hold? 

```{r}

Check.Int.Part <- semTools::compareFit(fit.loadings, fit.intercepts.part)
summary(Check.Int.Part)


```

```{r answer7, results = 'asis', echo = FALSE, include = printAnswers}

print.response("Yes, now the model does not fit significantly worse than the 
               model with fixed loadings. The chi-square difference test is 
               non-significant, and the changes in the CFI, RMSEA and SRMR after
               fixing the intercepts of all items except item 5 are smaller than
               .01, .015, and .015 respectively. In addition, the model with
               fixed loadings and intercept also has sufficient fit to the data.
               Partial intercept invariance therefore holds.")

```

## - What is the reliability per group for the final model? It is sufficient?

```{r}
# Calculate Reliability for the final model
compRelSEM(fit.intercepts.part)

```

```{r answer8, results = 'asis', echo = FALSE, include = printAnswers}

print.response("The reliability for group 1 is equal to `0.802` and the reliability for group 2 is equal to `0.795`. Thus, the reliability is okay.")

```


# Panel Data

## - Write syntax for the factor model for the panel data.

```{r}
LI_CM <- '

# Define the latent factors.
CM_1 =~ NA*T1_CM_01 + 1*T1_CM_01 + T1_CM_02 + T1_CM_03 + T1_CM_04 + T1_CM_05 
+ T1_CM_06 + T1_CM_07 + T1_CM_08

CM_2 =~ NA*T2_CM_01 + 1*T2_CM_01 + T2_CM_02 + T2_CM_03 + T2_CM_04 + T2_CM_05 
+ T2_CM_06 + T2_CM_07 + T2_CM_08

CM_3 =~ NA*T3_CM_01 + 1*T3_CM_01 + T3_CM_02 + T3_CM_03 + T3_CM_04 + T3_CM_05 
+ T3_CM_06 + T3_CM_07 + T3_CM_08

# Intercepts
T1_CM_01 ~ 1
T1_CM_02 ~ 1
T1_CM_03 ~ 1
T1_CM_04 ~ 1
T1_CM_05 ~ 1
T1_CM_06 ~ 1
T1_CM_07 ~ 1
T1_CM_08 ~ 1

T2_CM_01 ~ 1
T2_CM_02 ~ 1
T2_CM_03 ~ 1
T2_CM_04 ~ 1
T2_CM_05 ~ 1
T2_CM_06 ~ 1
T2_CM_07 ~ 1
T2_CM_08 ~ 1

T3_CM_01 ~ 1
T3_CM_02 ~ 1
T3_CM_03 ~ 1
T3_CM_04 ~ 1
T3_CM_05 ~ 1
T3_CM_06 ~ 1
T3_CM_07 ~ 1
T3_CM_08 ~ 1

# Unique Variances
T1_CM_01 ~~ T1_CM_01
T1_CM_02 ~~ T1_CM_02
T1_CM_03 ~~ T1_CM_03
T1_CM_04 ~~ T1_CM_04
T1_CM_05 ~~ T1_CM_05
T1_CM_06 ~~ T1_CM_06
T1_CM_07 ~~ T1_CM_07
T1_CM_08 ~~ T1_CM_08

T2_CM_01 ~~ T2_CM_01
T2_CM_02 ~~ T2_CM_02
T2_CM_03 ~~ T2_CM_03
T2_CM_04 ~~ T2_CM_04
T2_CM_05 ~~ T2_CM_05
T2_CM_06 ~~ T2_CM_06
T2_CM_07 ~~ T2_CM_07
T2_CM_08 ~~ T2_CM_08

T3_CM_01 ~~ T3_CM_01
T3_CM_02 ~~ T3_CM_02
T3_CM_03 ~~ T3_CM_03
T3_CM_04 ~~ T3_CM_04
T3_CM_05 ~~ T3_CM_05
T3_CM_06 ~~ T3_CM_06
T3_CM_07 ~~ T3_CM_07
T3_CM_08 ~~ T3_CM_08

# Latent Variable Means
CM_1 ~ 0*1
CM_2 ~ 0*1
CM_3 ~ 0*1

# Latent Variable Variances and Covariance
CM_1 ~~ 1*CM_1
CM_2 ~~ 1*CM_2
CM_3 ~~ 1*CM_3
'

```

## - Use the `cfa` function to fit the model to the data and set the argument `mimic` equal to "mplus".

```{r}
# Model fitting
fit_CM_Long <- cfa(LI_CM, data = Data_Cross_Panel, mimic = "mplus")

```

## - Use the `summary` function. Does the model fit well to the data?
Guidelines:



Look at **CFI** (should be > .90), **RMSEA** (should be < .08), 
**SRMR** (should be < .08), and **chi-square** (should be non-significant).

* The CFI (Comparative Fit Index), RMSEA (Root Mean Squared Error of 
Approximation), and SRMR (Standardized Root Mean Squared Error) are 
Approximate Fit Indices. They do not correct for sampling variability,
have no clear cut-offs for good fit, and assume multivariate normal data.
They are more qualitative measures of fit.


    + CFI: Looks at the improvement in fit relative to the independence model 
       which states that every variable has a mean and variance, but all are 
       unrelated (correlations are all 0).
       
    + RMSEA: Tests for close fit between data (observed (co)variances) 
       and model implied relations.
       
    + SRMR:  Looks at the (mean) covariance residuals. 


* Chi-square: Actual test with corresponding null-hypothesis.
It tests for the null-hypothesis of perfect fit (in the population), that is,
the model perfectly recovers the observed (co)variances, which
might be a little unrealistic. It is (unnecessarily) strict (especially for 
large sample size), assumes multivariate normality, and assumes a random 
sample. Moreover, it doesn't work well in case where there are high correlations 
and/or high percentages of unique variance. 
It also tends to miss misfit in single parameters (e.g., a single 
large covariance residual) and patterns of smaller misfit (many small 
residuals), AND a non-significant result does NOT mean a good model.

```{r}
# Getting the results
summary(fit_CM_Long, fit.measures = TRUE)

```

```{r answer1_l, results = 'asis', echo = FALSE, include = printAnswers}

print.response("The fit of the model to the data is bad. The Chi-square test
               is significant, the CFI is too low, and the RMSEA and SRMR are
               both too high. Possible reason for the misfit could have to do 
               with the nature of the data. We have repeated measures here and
               scores on one time-point are likely related to scores
               on a successive time-point, and possibly above and 
               beyond the dependence that is already there through the correlated
               factors at T1, T2, and T3. If you're bad at multiple-choice 
               questions at T1 for example, you probably still are at T2 and T3. 
               That is why you often see correlations added between the same 
               items at different (usually succesive) time-points in models for 
               panel data. When using the code below to test for longitudinal 
               invariance, these correlations are automatically added to the 
               model so we won't add them here. Do see if you can spot these 
               serial correlations in the generated code below however.")

```


## - Generate code for checking configural invariance across time.

```{r}
# Generate code for checking configural invariance across time

longFacNames <- list(CM = c("CM_1","CM_2","CM_3"))

mod.configural.long <-semTools::measEq.syntax(LI_CM, 
                                     longFacNames = longFacNames,
                                     data = Data_Cross_Panel, missing = "fiml")
# Now we use a new argument longFacNames. For details, see ?measEq.syntax

# Inspect the generated code
cat(as.character(mod.configural.long))

```

## - Fit the configural invariance model to the data.

```{r}
fit.configural.long <- lavaan::cfa(as.character(mod.configural.long), 
                              data = Data_Cross_Panel, 
                              fixed.x = FALSE)

```

## - Does configural invariance hold?

```{r}
# Check model fit using CFI, RMSEA, SRMR, and Chi-square again
summary(fit.configural.long, fit.measures = TRUE)

```

```{r answer2_l, results = 'asis', echo = FALSE, include = printAnswers}

print.response("The fit is not great because while the SRMR is lower than its 
               recommended value, all other indices indicate a lack of fit 
               (despite serial correlations between the same item at 
               succesive time-points now being part of the model). 
               This shows that at (at least) one of the three time-points the model 
               fits less well. We just have to figure out exactly what part of the 
               model does not fit well, and at what time-point(s). We'll try to 
               figure this out below using modification indices")

```


## - Check the modification indices. For each time-point separaterly, which residual covariances would be worth adding?

```{r}
# Obtain the modification indices
MIs <- modindices(fit.configural.long)

# Show the first 30
head(MIs[order(MIs$mi, decreasing = TRUE), 1:4], n = 30)
```

```{r answer3_l, results = 'asis', echo = FALSE, include = printAnswers}

print.response("
At T3 items 2 and 5, and 1 and 3 are pretty correlated (`T3_CM_02 ~~ T3_CM_05`, `T3_CM_01 ~~ T3_CM_03`).
At T2 items 1 and 2, and 3 and 6 are pretty correlated (`T2_CM_01	~~ T2_CM_02`, `T2_CM_03	~~ T2_CM_06`).
At T1 items 4 and 5, and 3 and 6 are pretty correlated (`T1_CM_04	~~ T1_CM_05`, `T1_CM_03 ~~ T1_CM_06`). We see that the residual correlation between items 3 and 6 is a suggested modification at both T1 and T2. Since we're measuring the same construct at all three time-points, making similar adjustments at each time-point is preferred. We therefore choose to add the residual correlation between items 3 and 6  at T1, T2, and T3.")

```


## - Add one residual covariance per time-point to the model.

```{r}
LI_CM2 <- '

# Define the latent factors.
CM_1 =~ NA*T1_CM_01 + 1*T1_CM_01 + T1_CM_02 + T1_CM_03 + T1_CM_04 + T1_CM_05 + T1_CM_06 + T1_CM_07 + T1_CM_08
CM_2 =~ NA*T2_CM_01 + 1*T2_CM_01 + T2_CM_02 + T2_CM_03 + T2_CM_04 + T2_CM_05 + T2_CM_06 + T2_CM_07 + T2_CM_08
CM_3 =~ NA*T3_CM_01 + 1*T3_CM_01 + T3_CM_02 + T3_CM_03 + T3_CM_04 + T3_CM_05 + T3_CM_06 + T3_CM_07 + T3_CM_08

# Intercepts
T1_CM_01 ~ 1
T1_CM_02 ~ 1
T1_CM_03 ~ 1
T1_CM_04 ~ 1
T1_CM_05 ~ 1
T1_CM_06 ~ 1
T1_CM_07 ~ 1
T1_CM_08 ~ 1

T2_CM_01 ~ 1
T2_CM_02 ~ 1
T2_CM_03 ~ 1
T2_CM_04 ~ 1
T2_CM_05 ~ 1
T2_CM_06 ~ 1
T2_CM_07 ~ 1
T2_CM_08 ~ 1

T3_CM_01 ~ 1
T3_CM_02 ~ 1
T3_CM_03 ~ 1
T3_CM_04 ~ 1
T3_CM_05 ~ 1
T3_CM_06 ~ 1
T3_CM_07 ~ 1
T3_CM_08 ~ 1

# Unique Variances
T1_CM_01 ~~ T1_CM_01
T1_CM_02 ~~ T1_CM_02
T1_CM_03 ~~ T1_CM_03
T1_CM_04 ~~ T1_CM_04
T1_CM_05 ~~ T1_CM_05
T1_CM_06 ~~ T1_CM_06
T1_CM_07 ~~ T1_CM_07
T1_CM_08 ~~ T1_CM_08

T2_CM_01 ~~ T2_CM_01
T2_CM_02 ~~ T2_CM_02
T2_CM_03 ~~ T2_CM_03
T2_CM_04 ~~ T2_CM_04
T2_CM_05 ~~ T2_CM_05
T2_CM_06 ~~ T2_CM_06
T2_CM_07 ~~ T2_CM_07
T2_CM_08 ~~ T2_CM_08

T3_CM_01 ~~ T3_CM_01
T3_CM_02 ~~ T3_CM_02
T3_CM_03 ~~ T3_CM_03
T3_CM_04 ~~ T3_CM_04
T3_CM_05 ~~ T3_CM_05
T3_CM_06 ~~ T3_CM_06
T3_CM_07 ~~ T3_CM_07
T3_CM_08 ~~ T3_CM_08

# Latent Variable Means
CM_1 ~ 0*1
CM_2 ~ 0*1
CM_3 ~ 0*1

# Latent Variable Variances and Covariance
CM_1 ~~ 1*CM_1
CM_2 ~~ 1*CM_2
CM_3 ~~ 1*CM_3


# Residual Covariances 
T1_CM_03 ~~ T1_CM_06
T2_CM_03 ~~ T2_CM_06
T3_CM_03 ~~ T3_CM_06
'
```

## - Fit the adjusted model to the data.

```{r}
# Generate code and inspect it
mod.configural.long2 <-semTools::measEq.syntax(LI_CM2, 
                                              longFacNames = longFacNames,
                                              data = Data_Cross_Panel, missing = "fiml")
cat(as.character(mod.configural.long2))

# Model fitting
fit.configural.long2 <- lavaan::cfa(as.character(mod.configural.long2), 
                              data = Data_Cross_Panel, 
                              fixed.x = FALSE)
```

## - Does configural invariance hold with the added residual variances?

```{r}
# Check model fit using CFI, RMSEA, SRMR, and Chi-square again
summary(fit.configural.long2, fit.measures = TRUE)
```

```{r answer4_l, results = 'asis', echo = FALSE, include = printAnswers}

print.response("The fit is still not great because, apart from the SRMR, all 
               fit indices are still indicating model misfit. We will therefore
               make some further alterations based on the modification indices.
               Note that you should always be very careful with the post-hoc 
               model adjustments based on modification indices. Any alteration
               needs to make substantive sense, and purely adjusting your model 
               based on fit indices means the risk of overfitting your model to 
               the current sample, instead of finding a proper general model 
               that also applies to (hypothetical) future samples from the same
               population.")

```

## - Check the modification indices. For each time-point separaterly, which residual covariances would be worth adding?

```{r}
# Obtain the modification indices
MIs2 <- modindices(fit.configural.long2)

# Show the first 30
head(MIs2[order(MIs2$mi, decreasing = TRUE), 1:4], n = 30)
```

```{r answer5_l, results = 'asis', echo = FALSE, include = printAnswers}

print.response("
 At T3 items 1 and 3, 3 and 5, and 1 and 2 are pretty correlated (`T3_CM_01 ~~ T3_CM_03`, `T3_CM_03 ~~ T3_CM_05`, `T3_CM_01 ~~ T3_CM_02`).
 At T2 items 1 and 2, 2 and 6, and 2 and 3 are pretty correlated (`T2_CM_01	~~ T2_CM_02`, `T2_CM_02	~~ T2_CM_06`, `T2_CM_02	~~ T2_CM_03` ).
 At T1 items 4 and 5 are pretty correlated (`T1_CM_04	~~ T1_CM_05`). 
 Now we see that the residual correlation between items 1 and 2 is a suggested modification at both T3 and T2. Since we want to make similar adjustments at each time-point, we choose to add the residual correlation between items 1 and 2  at T1, T2, and T3.
  ")

```

## - Add one additional residual covariance per time-point to the model.

```{r}
LI_CM3 <- '

# Define the latent factors.
CM_1 =~ NA*T1_CM_01 + 1*T1_CM_01 + T1_CM_02 + T1_CM_03 + T1_CM_04 + T1_CM_05 
+ T1_CM_06 + T1_CM_07 + T1_CM_08
CM_2 =~ NA*T2_CM_01 + 1*T2_CM_01 + T2_CM_02 + T2_CM_03 + T2_CM_04 + T2_CM_05 
+ T2_CM_06 + T2_CM_07 + T2_CM_08
CM_3 =~ NA*T3_CM_01 + 1*T3_CM_01 + T3_CM_02 + T3_CM_03 + T3_CM_04 + T3_CM_05 
+ T3_CM_06 + T3_CM_07 + T3_CM_08

# Intercepts
T1_CM_01 ~ 1
T1_CM_02 ~ 1
T1_CM_03 ~ 1
T1_CM_04 ~ 1
T1_CM_05 ~ 1
T1_CM_06 ~ 1
T1_CM_07 ~ 1
T1_CM_08 ~ 1

T2_CM_01 ~ 1
T2_CM_02 ~ 1
T2_CM_03 ~ 1
T2_CM_04 ~ 1
T2_CM_05 ~ 1
T2_CM_06 ~ 1
T2_CM_07 ~ 1
T2_CM_08 ~ 1

T3_CM_01 ~ 1
T3_CM_02 ~ 1
T3_CM_03 ~ 1
T3_CM_04 ~ 1
T3_CM_05 ~ 1
T3_CM_06 ~ 1
T3_CM_07 ~ 1
T3_CM_08 ~ 1

# Unique Variances
T1_CM_01 ~~ T1_CM_01
T1_CM_02 ~~ T1_CM_02
T1_CM_03 ~~ T1_CM_03
T1_CM_04 ~~ T1_CM_04
T1_CM_05 ~~ T1_CM_05
T1_CM_06 ~~ T1_CM_06
T1_CM_07 ~~ T1_CM_07
T1_CM_08 ~~ T1_CM_08

T2_CM_01 ~~ T2_CM_01
T2_CM_02 ~~ T2_CM_02
T2_CM_03 ~~ T2_CM_03
T2_CM_04 ~~ T2_CM_04
T2_CM_05 ~~ T2_CM_05
T2_CM_06 ~~ T2_CM_06
T2_CM_07 ~~ T2_CM_07
T2_CM_08 ~~ T2_CM_08

T3_CM_01 ~~ T3_CM_01
T3_CM_02 ~~ T3_CM_02
T3_CM_03 ~~ T3_CM_03
T3_CM_04 ~~ T3_CM_04
T3_CM_05 ~~ T3_CM_05
T3_CM_06 ~~ T3_CM_06
T3_CM_07 ~~ T3_CM_07
T3_CM_08 ~~ T3_CM_08

# Latent Variable Means
CM_1 ~ 0*1
CM_2 ~ 0*1
CM_3 ~ 0*1

# Latent Variable Variances and Covariance
CM_1 ~~ 1*CM_1
CM_2 ~~ 1*CM_2
CM_3 ~~ 1*CM_3


# Residual Covariances 
T1_CM_03 ~~ T1_CM_06
T2_CM_03 ~~ T2_CM_06
T3_CM_03 ~~ T3_CM_06

T1_CM_01 ~~ T1_CM_02
T2_CM_01 ~~ T2_CM_02
T3_CM_01 ~~ T3_CM_02

'
```

## - Fit the adjusted model to the data.

```{r}
# Generate code and inspect it
mod.configural.long3 <-semTools::measEq.syntax(LI_CM3, 
                                               longFacNames = longFacNames,
                                               data = Data_Cross_Panel, missing = "fiml")
cat(as.character(mod.configural.long3))

# Model fitting
fit.configural.long3 <- lavaan::cfa(as.character(mod.configural.long3), 
                               data = Data_Cross_Panel, 
                               fixed.x = FALSE)
```

## - Does configural invariance hold with the additional added residual variances?

```{r}
# Check model fit using CFI, RMSEA, SRMR, and Chi-square again
summary(fit.configural.long3, fit.measures = TRUE)
```

```{r answer6_l, results = 'asis', echo = FALSE, include = printAnswers}

print.response("The fit is still not good enough, even after adding additional 
               residual correlations. Looking at the modification indices you 
               might have noticed however that items 3, 1, and 6 are always the
               ones causing 'problems'. We will therefore remove these items one
               at a time, but the same caution we made above with adding 
               covariances based on modification indices apply here! Model 
               modifications should ideally not be based on statistics alone and 
               researchers always want to be careful when removing items as this
               might change the interpretation and/or scope of the latent 
               variable being modeled.")

```

## - Remove item 3 from the original model (i.e., without any added residual covariances).

```{r}
LI_CM4 <- '

# Define the latent factors.
CM_1 =~ NA*T1_CM_01 + 1*T1_CM_01 + T1_CM_02 + T1_CM_04 + T1_CM_05 + T1_CM_06 
+ T1_CM_07 + T1_CM_08
CM_2 =~ NA*T2_CM_01 + 1*T2_CM_01 + T2_CM_02 + T2_CM_04 + T2_CM_05 + T2_CM_06 
+ T2_CM_07 + T2_CM_08
CM_3 =~ NA*T3_CM_01 + 1*T3_CM_01 + T3_CM_02 + T3_CM_04 + T3_CM_05 + T3_CM_06 
+ T3_CM_07 + T3_CM_08

# Intercepts
T1_CM_01 ~ 1
T1_CM_02 ~ 1
T1_CM_04 ~ 1
T1_CM_05 ~ 1
T1_CM_06 ~ 1
T1_CM_07 ~ 1
T1_CM_08 ~ 1

T2_CM_01 ~ 1
T2_CM_02 ~ 1
T2_CM_04 ~ 1
T2_CM_05 ~ 1
T2_CM_06 ~ 1
T2_CM_07 ~ 1
T2_CM_08 ~ 1

T3_CM_01 ~ 1
T3_CM_02 ~ 1
T3_CM_04 ~ 1
T3_CM_05 ~ 1
T3_CM_06 ~ 1
T3_CM_07 ~ 1
T3_CM_08 ~ 1

# Unique Variances
T1_CM_01 ~~ T1_CM_01
T1_CM_02 ~~ T1_CM_02
T1_CM_04 ~~ T1_CM_04
T1_CM_05 ~~ T1_CM_05
T1_CM_06 ~~ T1_CM_06
T1_CM_07 ~~ T1_CM_07
T1_CM_08 ~~ T1_CM_08

T2_CM_01 ~~ T2_CM_01
T2_CM_02 ~~ T2_CM_02
T2_CM_04 ~~ T2_CM_04
T2_CM_05 ~~ T2_CM_05
T2_CM_06 ~~ T2_CM_06
T2_CM_07 ~~ T2_CM_07
T2_CM_08 ~~ T2_CM_08

T3_CM_01 ~~ T3_CM_01
T3_CM_02 ~~ T3_CM_02
T3_CM_04 ~~ T3_CM_04
T3_CM_05 ~~ T3_CM_05
T3_CM_06 ~~ T3_CM_06
T3_CM_07 ~~ T3_CM_07
T3_CM_08 ~~ T3_CM_08

# Latent Variable Means
CM_1 ~ 0*1
CM_2 ~ 0*1
CM_3 ~ 0*1

# Latent Variable Variances and Covariance
CM_1 ~~ 1*CM_1
CM_2 ~~ 1*CM_2
CM_3 ~~ 1*CM_3
'
```

## - Fit the adjusted model to the data.

```{r}
# Generate code and inspect it
mod.configural.long4 <- semTools::measEq.syntax(LI_CM4, 
                                               longFacNames = longFacNames,
                                               data = Data_Cross_Panel, missing = "fiml")
cat(as.character(mod.configural.long4))

# Model fitting
fit.configural.long4 <- lavaan::cfa(as.character(mod.configural.long4), 
                               data = Data_Cross_Panel, 
                               fixed.x = FALSE)
```


## - Does configural invariance hold after removing item 3?

```{r}
# Check model fit using CFI, RMSEA, SRMR, and Chi-square again
summary(fit.configural.long4, fit.measures=T)
```

```{r answer7_l, results = 'asis', echo = FALSE, include = printAnswers}

print.response("The fit is still not good.")

```

## - Remove item 1 in addition to item 3.

```{r}
LI_CM5 <- '

# Define the latent factors.
CM_1 =~ NA*T1_CM_02 + T1_CM_02 + T1_CM_04 + T1_CM_05 + T1_CM_06 + T1_CM_07 
+ T1_CM_08
CM_2 =~ NA*T2_CM_02 + T2_CM_02 + T2_CM_04 + T2_CM_05 + T2_CM_06 + T2_CM_07 
+ T2_CM_08
CM_3 =~ NA*T3_CM_02 + T3_CM_02 + T3_CM_04 + T3_CM_05 + T3_CM_06 + T3_CM_07 
+ T3_CM_08

# Intercepts
T1_CM_02 ~ 1
T1_CM_04 ~ 1
T1_CM_05 ~ 1
T1_CM_06 ~ 1
T1_CM_07 ~ 1
T1_CM_08 ~ 1

T2_CM_02 ~ 1
T2_CM_04 ~ 1
T2_CM_05 ~ 1
T2_CM_06 ~ 1
T2_CM_07 ~ 1
T2_CM_08 ~ 1

T3_CM_02 ~ 1
T3_CM_04 ~ 1
T3_CM_05 ~ 1
T3_CM_06 ~ 1
T3_CM_07 ~ 1
T3_CM_08 ~ 1

# Unique Variances
T1_CM_02 ~~ T1_CM_02
T1_CM_04 ~~ T1_CM_04
T1_CM_05 ~~ T1_CM_05
T1_CM_06 ~~ T1_CM_06
T1_CM_07 ~~ T1_CM_07
T1_CM_08 ~~ T1_CM_08

T2_CM_02 ~~ T2_CM_02
T2_CM_04 ~~ T2_CM_04
T2_CM_05 ~~ T2_CM_05
T2_CM_06 ~~ T2_CM_06
T2_CM_07 ~~ T2_CM_07
T2_CM_08 ~~ T2_CM_08

T3_CM_02 ~~ T3_CM_02
T3_CM_04 ~~ T3_CM_04
T3_CM_05 ~~ T3_CM_05
T3_CM_06 ~~ T3_CM_06
T3_CM_07 ~~ T3_CM_07
T3_CM_08 ~~ T3_CM_08

# Latent Variable Means
CM_1 ~ 0*1
CM_2 ~ 0*1
CM_3 ~ 0*1

# Latent Variable Variances and Covariance
CM_1 ~~ 1*CM_1
CM_2 ~~ 1*CM_2
CM_3 ~~ 1*CM_3
'
```

## - Fit the adjusted model to the data.

```{r}
# Generate code and inspect it
mod.configural.long5 <-semTools::measEq.syntax(LI_CM5, 
                                               longFacNames = longFacNames,
                                               data = Data_Cross_Panel, missing = "fiml")
cat(as.character(mod.configural.long5))

# Model fitting
fit.configural.long5 <- lavaan::cfa(as.character(mod.configural.long5), 
                               data = Data_Cross_Panel, 
                               fixed.x = FALSE)
```


## - Does configural invariance hold after removing items 3 and 1?

```{r}
# Check model fit using CFI, RMSEA, SRMR, and Chi-square again
summary(fit.configural.long5, fit.measures=T)
```

```{r answer8_l, results = 'asis', echo = FALSE, include = printAnswers}

print.response("The fit is gettin better (with the point estimate of the RMSEA 
               on the boundary of its cut-off value now, although the upper-boundary
               of its 95% CI is still too high), but it's not good yet.")

```

## - Remove item 6 in addition to items 3 and 1.

```{r}
LI_CM6 <- '

# Define the latent factors.
CM_1 =~ NA*T1_CM_02 + T1_CM_02 + T1_CM_04 + T1_CM_05 + T1_CM_07 + T1_CM_08
CM_2 =~ NA*T2_CM_02 + T2_CM_02 + T2_CM_04 + T2_CM_05 + T2_CM_07 + T2_CM_08
CM_3 =~ NA*T3_CM_02 + T3_CM_02 + T3_CM_04 + T3_CM_05 + T3_CM_07 + T3_CM_08

# Intercepts
T1_CM_02 ~ 1
T1_CM_04 ~ 1
T1_CM_05 ~ 1
T1_CM_07 ~ 1
T1_CM_08 ~ 1

T2_CM_02 ~ 1
T2_CM_04 ~ 1
T2_CM_05 ~ 1
T2_CM_07 ~ 1
T2_CM_08 ~ 1

T3_CM_02 ~ 1
T3_CM_04 ~ 1
T3_CM_05 ~ 1
T3_CM_07 ~ 1
T3_CM_08 ~ 1

# Unique Variances
T1_CM_02 ~~ T1_CM_02
T1_CM_04 ~~ T1_CM_04
T1_CM_05 ~~ T1_CM_05
T1_CM_07 ~~ T1_CM_07
T1_CM_08 ~~ T1_CM_08

T2_CM_02 ~~ T2_CM_02
T2_CM_04 ~~ T2_CM_04
T2_CM_05 ~~ T2_CM_05
T2_CM_07 ~~ T2_CM_07
T2_CM_08 ~~ T2_CM_08

T3_CM_02 ~~ T3_CM_02
T3_CM_04 ~~ T3_CM_04
T3_CM_05 ~~ T3_CM_05
T3_CM_07 ~~ T3_CM_07
T3_CM_08 ~~ T3_CM_08

# Latent Variable Means
CM_1 ~ 0*1
CM_2 ~ 0*1
CM_3 ~ 0*1

# Latent Variable Variances and Covariance
CM_1 ~~ 1*CM_1
CM_2 ~~ 1*CM_2
CM_3 ~~ 1*CM_3
'
```

## - Fit the adjusted model to the data.

```{r}
# Generate code and inspect it
mod.configural.long6 <-semTools::measEq.syntax(LI_CM6, 
                                               longFacNames = longFacNames,
                                               data=Data_Cross_Panel, missing = "fiml")
cat(as.character(mod.configural.long6))

# Model fitting
fit.configural.long6 <- lavaan::cfa(as.character(mod.configural.long6), 
                               data = Data_Cross_Panel, 
                               fixed.x = FALSE)
```


## - Does configural invariance hold after removing items 3, 1, and 6?

```{r}
# Check model fit using CFI, RMSEA, SRMR, and Chi-square again
summary(fit.configural.long6, fit.measures = TRUE)
```

```{r answer9_l, results = 'asis', echo = FALSE, include = printAnswers}

print.response("Yes, now we have good fit. Only the chi-square test is still 
               significant, and the upper-boundary of the 95% CI of the RMSEA is
               still too high, but just barely so. Now that we have a good 
               fitting configural invariance model, we are ready to start 
               constraining loadings and intercepts across time-points")

```


## - Generate code for checking loading invariance between groups.

```{r}
# Generate code for checking longitudinal invariance of loadings

mod.loadings.long <-semTools::measEq.syntax(LI_CM6, longFacNames = longFacNames, 
                                     long.equal = "loadings", data=Data_Cross_Panel,
                                     missing="fiml")

# Inspect the generated code
cat(as.character(mod.loadings.long))
```

## - Generate code for checking intercept invariance between groups.

```{r}
# Generate code for checking longitudinal invariance of intercepts and loadings
mod.int.load.long <-semTools::measEq.syntax(LI_CM6, 
                                              longFacNames = longFacNames, 
                                              long.equal = c("intercepts", 
                                                             "loadings"), 
                                              data=Data_Cross_Panel, missing="fiml")

# Inspect the generated code
cat(as.character(mod.int.load.long))
```

## - Fit the three models to the data.

```{r}
# Fit the loading invariance model
fit.loading.long <- lavaan::cfa(as.character(mod.loadings.long), 
                               data = Data_Cross_Panel, 
                               fixed.x=FALSE)

# Fit the intercept invariance model
fit.int.load.long <- lavaan::cfa(as.character(mod.int.load.long), 
                                  data = Data_Cross_Panel, 
                                  fixed.x=FALSE)

```

## - Now compare the different models successively, using the guidelines below.

* More than 20 observations per group: Change should not be larger than -.01 for **CFI**, 0.015 for **RMSEA**, and 0.03 for **SRMR** (*loading invariance*) or .015 for **SRMR** (*intercept invariance*). Difference in chi-square should be non-significant

* 10-20 observations per group: All as above but: Change should not be larger than -.02 for **CFI** and 0.03 **RMSEA** for *loading invariance*. 

* Less than 10 observations per group: Invariance testing is not a good idea.

## - Does loading invariance hold?

```{r}
Check.Load.Long <- semTools::compareFit(fit.configural.long6, fit.loading.long) 
summary(Check.Load.Long)

```

```{r answer10_l, results = 'asis', echo = FALSE, include = printAnswers}

print.response("Yes, loading invariance holds because while the chi-square 
               difference test is significant, the changes in all other 
               fit indices are smaller than the maximum allowed changes 
               mentioned in the guidelines.")

```

## - Does intercept invariance hold?

```{r}
Check.Int.Load.Long <- semTools::compareFit(fit.loading.long, 
                                            fit.int.load.long)
summary(Check.Int.Load.Long) 

```

```{r answer11_l, results = 'asis', echo = FALSE, include = printAnswers}

print.response("This is a bit or a borderline-case, the chi-square difference 
               test is significant and the change is CFI is juuust to large (by
               .001). We could now see which item has the largest change in 
               intercept value across time, just like we did above for 
               cross-sectional data, but for this assignment we'll settle for
               this model and conclude that intercept invariance holds.")

```

## - What is the reliability per time-point for the final model? It is sufficient?

```{r}
# Calculate Reliability for the final model
compRelSEM(fit.int.load.long)


```

```{r answer12_l, results = 'asis', echo = FALSE, include = printAnswers}

print.response("The reliability for all time-points are a bit low (with `0.640` for T1, `0.687` for T2 and `0.637` for T3).")

```

# Intensive Longitudinal Data

## - Write syntax for the multilevel factor model for the ILD.

```{r}
Mutlilevel_factor <- '
# Define the latent factors on level 1.
level: 1
  DG_1 =~ NA*anything +lambda1*anything + fail + fault + smart + 
        mistakes + hire + looks

# Intercepts
  anything ~ i1*1
  fail ~ 1
  fault ~ 1
  smart ~ 1
  mistakes ~ 1
  hire ~ 1
  looks ~ 1
  
# Unique Variances
  anything ~~ anything
  fail ~~ fail
  fault ~~ fault
  smart ~~ smart
  mistakes ~~ mistakes
  hire ~~ hire
  looks ~~ looks
  

# Latent Variable Means
  DG_1 ~ 0*1

# Latent Variable Variances and Covariance
  DG_1 ~~ 1*DG_1
  
# Define the latent factors on level 2.
level: 2

  DG_1 =~ NA*anything +lambda1*anything + fail + fault + smart + 
        mistakes + hire + looks
        
# Latent Variable Means
  DG_1 ~ 0*1

# Latent Variable Variances and Covariance
  DG_1 ~~ 1*DG_1
'
```


## - Use the `cfa` function to fit the model to the data and set the argument `mimic` equal to "mplus".

```{r}
# Model fitting
fit.multilevel <- cfa(model = Mutlilevel_factor, data = Data_ILD, 
                      cluster = "id")

```

## - Use the `summary` function. Does the model fit well to the data? Look at the fit for the two levels separately.

Guidelines:



Look at **CFI** (should be > .90), **RMSEA** (should be < .08), 
**SRMR** (should be < .08), and **chi-square** (should be non-significant).

* The CFI (Comparative Fit Index), RMSEA (Root Mean Squared Error of 
Approximation), and SRMR (Standardized Root Mean Squared Error) are 
Approximate Fit Indices. They do not correct for sampling variability,
have no clear cut-offs for good fit, and assume multivariate normal data.
They are more qualitative measures of fit.


    + CFI: Looks at the improvement in fit relative to the independence model 
       which states that every variable has a mean and variance, but all are 
       unrelated (correlations are all 0).
       
    + RMSEA: Tests for close fit between data (observed (co)variances) 
       and model implied relations.
       
    + SRMR:  Looks at the (mean) covariance residuals. 


* Chi-square: Actual test with corresponding null-hypothesis.
It tests for the null-hypothesis of perfect fit (in the population), that is,
the model perfectly recovers the observed (co)variances, which
might be a little unrealistic. It is (unnecessarily) strict (especially for 
large sample size), assumes multivariate normality, and assumes a random 
sample. Moreover, it doesn't work well in case where there are high correlations 
and/or high percentages of unique variance. 
It also tends to miss misfit in single parameters (e.g., a single 
large covariance residual) and patterns of smaller misfit (many small 
residuals), AND a non-significant result does NOT mean a good model.

```{r}
# Check model fit using CFI, RMSEA, SRMR, and Chi-square again
summary(fit.multilevel, fit.measures = TRUE) 

```

```{r answer1_il, results = 'asis', echo = FALSE, include = printAnswers}

print.response("The fit is not completely okay because both the chi-square test 
               and the CFI indicate that the model does not describe the data 
               properly. The RMSEA and the SRMR are okay according to our
               guidelines but as we mentioned above we want 3 or more of our 
               indices to indicate good fit. Notice btw how you get an SRMR for
               both the within- and between-person model separately. You can also 
               get/calculate other fit indices per level, but lavaan does not 
               provide other indices, and we won't go into it further here. The
               separate SRMRs are a good reminder that we really are looking at 
               two different models measuring two different things here, and that
               each model needs to fit its data, that is, the within- and 
               between-person (co)variance matrix respectively, properly. Below
               we will calculate the ICC (a measure of the amount of variance
               on the between-level) and ask lavaan for the within- and 
               between-person variance-covariance matrices so you can see how
               different the data that the models are fit to is. After that, we
               will use modification indices again to see how we can (carefully) adjust 
               our model to improve model fit.")

```

## - Check the proportion of variance in the indicators that is on the between level using the ICC.

```{r}
lavInspect(fit.multilevel, "icc")

```

```{r answer2_il, results = 'asis', echo = FALSE, include = printAnswers}

print.response("The ICC tell us what proportion of the total variance of a 
               variable is variance on the between-person level. Here we see that the 
               percentage of level 2 variance varies from 32.50% for 'smart' to
               36.80% for 'looks'. It is this between-person variance (and the 
               between-person covariances) that the between-model is fit to.
               Below we will ask for the entire within-person and between-person
               (co)variance matrices. The ICC is based on the numbers on the 
               diagonal of these two matrices.")

```

## - Retrieve the within- and between-person covariance matrices.

```{r}
lavInspect(fit.multilevel, "h1")
```

## - Check the modification indices to see how we can improve model fit.

```{r}
# Obtain the modification indices
MIsML <- modindices(fit.multilevel)

# Show the first 20
head(MIsML[order(MIsML$mi, decreasing = TRUE), 1:7], n = 30)
```

```{r answer3_il, results = 'asis', echo = FALSE, include = printAnswers}

print.response("The modification indices tell us that the biggest improvement in
               model fit will result from adding a residual covariance between
               'anything' and 'fail' on level 1. Notice that keeping track of
               what level a change needs to be made to is important here.")

```

## - Add the residual covariance between 'anything' and 'fail' on level 1

```{r}
Mutlilevel_factor2 <- '
# Define the latent factors on level 1.
level: 1
  DG_1 =~ NA*anything +lambda1*anything + fail + fault + smart + 
        mistakes + hire + looks

# Intercepts
  anything ~ i1*1
  fail ~ 1
  fault ~ 1
  smart ~ 1
  mistakes ~ 1
  hire ~ 1
  looks ~ 1
  
# Unique Variances
  anything ~~ anything
  fail ~~ fail
  fault ~~ fault
  smart ~~ smart
  mistakes ~~ mistakes
  hire ~~ hire
  looks ~~ looks
  

# Latent Variable Means
  DG_1 ~ 0*1

# Latent Variable Variances and Covariance
  DG_1 ~~ 1*DG_1
  
# Residual Covariances

 anything ~~ fail
  
# Define the latent factors on level 2.
level: 2

  DG_1 =~ NA*anything +lambda1*anything + fail + fault + smart + 
        mistakes + hire + looks
        
# Latent Variable Means
  DG_1 ~ 0*1

# Latent Variable Variances and Covariance
  DG_1 ~~ 1*DG_1
'
```

## - Fit the adjusted model to the data.

```{r}
# Model fitting
fit.multilevel2 <- cfa(model = Mutlilevel_factor2, data = Data_ILD, 
                      cluster = "id")
```

## - Does the model fit now?

```{r}
summary(fit.multilevel2, fit.measures=T)
```

```{r answer4_il, results = 'asis', echo = FALSE, include = printAnswers}

print.response("Now the model fits well. The chi-square is stil significant, but
the CFI, RMSEA, and SRMR all indicate sufficient fit to the data.  The SRMR for 
the between-level is a little too high, but for this assignment we'll leave it. 
With your own data you could always check the modification indices to see
what change to the between-person model would benefit fit the most. Notice 
that the between-person SRMR changed after we added the residual covariance
to the within-person model. It is good to be aware that a change on any level
of the model can influence the model as a whole. There is alot of moving parts
in multilevel models, even more so than in normal SEM models, so you need to 
stay vigilant.")

```


## - What is the reliability per time-point for the final model? Is it sufficient?

```{r}
# Calculate Reliability per level
compRelSEM(fit.multilevel2)

```

```{r answer5_il, results = 'asis', echo = FALSE, include = printAnswers}

print.response("The reliability is not great (with `0.603` for the within level and `0.707` for the between level. Instead of looking at the reliabilities of the within and between factor models, we could also look at the reliability of composite (e.g. sum) scores of the mean-item scores (level 2 composite) and person-centered item scores (level 1
# composite). See  https://doi.org/10.1037/met0000287.")

```


# Latent Markov Factor Analysis

After successfully installing and loading the `lmfa` package, you can perform LMFA by means of the three-step estimation approach. The package consists of three main functions that are shown below. For details about the function arguments, see the function documentations, which can be opened with `?[functionname]`

Research Questions we want to answer:

1. How many measurement models (MMs) are underlying our ILD? 

2. How do the MMs differ?

3. How do subjects transition between the MMs over time, and is this related to time- or subject-specific covariates?

4. For which subjects does longitudinal invariance hold, and for which of these subjects does invariance hold across subjects?


## -Step 1

The step 1 function estimates the state-specific factor analysis models (with or without model selection). For our example data, we estimated models with one to four states and two to three factors per state. The estimation time takes around three hours. Therefore, you can just load the model selection object, which is provided together with the package.


```{r, eval = FALSE}
# Don't run this; it takes three hours
set.seed(1000)
modelselection <- step1(data = ESM,
                        indicators = c(
                          "Interested","Joyful","Determined","Calm","Lively",
                          "Enthusiastic","Relaxed","Cheerful","Content",
                          "Energetic","Upset","Gloomy","Sluggish","Anxious",
                          "Bored","Irritated","Nervous","Listless"),
                        modelselection = TRUE,
                        n_state_range = 1:4,
                        n_fact_range = 2:3,
                        n_starts = 25,
                        max_iterations = 1000)

```


```{r}
# Load the step-1 results
data("modelselection")

# Obtain the model-selection results
summary(modelselection)

```

For model selection you can also look at two outputs, one for the BIC and one for the CHull:

```{r}
# BIC
plot(modelselection)

# CHull
chull_lmfa(modelselection)

```

We choose model [323] and then extract it from the results and display the parameter estimates. We can see differences in all parameters across states.

```{r}
# Extract the results
measurementmodel323 <- modelselection$`[323]`

# Parameter estimates
summary(measurementmodel323)

```

We can also obtain copy of the dataset with the state-specific factor scores attached.

```{r}
# Obtain data with factor scores attached
ESM_fs <- factorscores_lmfa(data = ESM, model = measurementmodel323)

```

## -Step 2

The step 2 function obtains the posterior state-membership probabilities and the model state assignments and calculates the classification error. We don't have to look into it too much. We mainly need the object to obtain correct estimates in step 3. In brief: The R-square measure R_entropy indicates how much the measurement models differ and thus how well the states are separated (from 0 = bad separation to 1= good separation). 

```{r}
# Obtain classifications and error
classification <- step2(data = ESM_fs, model = measurementmodel323)

# Print he results
summary(classification)

```

The R-entropy for our example indicates good separation. This explains the small total classification error.

## -Step 3

The step 3 function estimates the transitions between the states (conditional on covariates) by means of a continuous-time latent Markov model. We include two predictors, *having had an intervention* (yes or no) and *unpleasantness of the most unpleasant event* (0-100). The estimation for our example data took about 20 minutes. Again, you can just load the object from the data. 

We first look at the Wald tests to see if the inclusion of the covariates is significant. If you don't like p-values, you may (additionally) run  alternative models (e.g., with only one covariate or no covariate) and compare the BIC values and choose the one with the lowest. For this example it didn't make a difference but it may make a difference for other data. We decide to keep both covariates in the model.

```{r, eval = FALSE}
set.seed(1000)
transitionmodel <- step3(data = ESM_fs,
                         identifier = "id",
                         n_state = 3,
                         postprobs =
                           classification$classification_posteriors[,-1],
                         timeintervals  = "deltaT",
                         initialCovariates = NULL,
                         transitionCovariates =
                           c("intervention", "negativeEvent"),
                         n_starts = 25,
                         max_iterations = 1000)
```

```{r}
# Load the step-3 results
data("transitionmodel")

# Obtain the transition-model results
summary(transitionmodel)
```

The parameters are difficult to interpret. It is better to obtain the probabilities for an interval of interest and different covariate values that you want to compare with. For example, keep the *negative event* score equal to it's sample mean 49.65 and obtain the probabilities for *before having had an intervention* and for *after having had an intervention*; both for a unit interval.

Comparing the transition probabilities, we see that *having had an intervention* is related to relatively smaller probabilities of transitioning to   *displeasure state* ("S1") as well as relative smaller probabilities of staying in the *displeasure state* ("S1"). 

```{r}
# Obtain probabilities
probabilities(model = transitionmodel, 
              deltaT = 1,
              initialCovariateScores = NULL,
              transitionCovariateScores = c(0,49.65))

probabilities(model = transitionmodel, 
              deltaT = 1,
              initialCovariateScores = NULL,
              transitionCovariateScores = c(1,49.65))
```

We can now obtain a copy of our dataset with the final state assignments attached.

```{r}
# Attach updates classifications to data
ESM_fs_cl <- transitionmodel$data

```

To obtain an initial idea of how subjects transition, we can inspect transition plots (here only for four subjects).

```{r}
# Obtain transition plots
layout(matrix(c(1,3,2,4), nrow = 2, ncol = 2))
for(i in 1:4) plot(transitionmodel, identifier = "id", id = i)

```

Finally, we can get some insights into for which subjects longitudinal invariance holds, and for which subjects the model is also invariant across subjects. In other words, we can get insight into which subjects stay in the same state for their entire participation, and if there are subjects that are in the same permanent state. If there were subjects for which longitudinal invariance holds, they would be listed under their permanent state membership in the output below. For subjects who are in the same permanent state, invariance holds across all observations. However, the NA's indicate that this longitudinal invariance doesn't apply to any of the subjects.

```{r}
# Obtain invariance information
invariance(model = transitionmodel, identifier = "id")

```



## -Summary of the findings

1. How many MMs are underlying our ILD? 

* *Three MMs are underlying the example data.*

2. How do the MMs differ?

* *The number and nature of the factors differ, implying that configural invariance is violated for our example data. More specifically, we found three states (a displeasure, a neutral, and a pleasure state) that all contained a positive affect and a negative affect (or distress) factor, but the displeasure state was additionally characterized by a drive factor and the neutral state by a serenity factor.*

3. How do subjects transition between the MMs over time, and is this related 
to time- or subject-specific covariates?

* *Most subjects started in the displeasure state. The probabilities of staying in a state were generally higher than transitioning to another state (especially for subjects in the displeasure state). Transitions to the displeasure state were most likely, especially when experiencing negative events. After receiving an intervention, the probabilities of transitioning to and staying in the neutral or pleasure state increased.* 

4. For which subjects does longitudinal invariance hold, and for
which of these subjects does invariance hold across subjects? 

* *Longitudinal invariance does not hold for any of the subjects and, therefore, neither does invariance across subjects.*



<!-- Attach full code at the end of the solutions script -->
```{r sol-title, results = 'asis', echo = FALSE}
cat(
     "# TL;DR, just give me the code!"
)

```



```{r TLDR, ref.label = knitr::all_labels()[!knitr::all_labels() %in% c("global_options", "klippy", "sol-title", "answer1","answer2","answer3","answer4","answer5","answer6", "answer7", "answer8", "answer1_l", "answer2_l", "answer3_l", "answer4_l", "answer5_l", "answer6_l", "answer7_l", "answer8_l", "answer9_l", "answer10_l", "answer11_l", "answer12_l", "answer1_il", "answer2_il", "answer3_il", "answer4_il", "answer5_il")], echo = TRUE, eval = FALSE}
```